{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "        metal_text = (' '.join(words))\n",
    "\n",
    "    return metal_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please the word: dragon from the sky\n",
      "dragon from the sky is no longer to the world to the world is\n",
      "i can never be a man ,\n",
      "i'm my head to be, i'm my head\n",
      "i am\n",
      "and i'm the last man\n",
      "i don't believe, i'm a man and\n",
      "you have been, i'm no one and you are\n",
      "the night is my mind and you will see you to me to be the one\n",
      "i'm my mind\n",
      "and we got a new\n",
      "the one can you see the end, i'm not, the land\n",
      "the last\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with open('dictionaries' + '.pkl', 'rb') as f:\n",
    "    diction =  pickle.load(f)\n",
    "seq_size = diction[0]\n",
    "n_vocab = diction[1]\n",
    "vocab_to_int = diction[2]\n",
    "embedding_size = diction[3]\n",
    "lstm_size = diction[4]\n",
    "int_to_vocab = diction[5]\n",
    "\n",
    "net = RNNModule(n_vocab, seq_size,\n",
    "            embedding_size, lstm_size)\n",
    "\n",
    "net.load_state_dict(torch.load('LSTM_gener'))\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Note: Python 2.x users should use raw_input, the equivalent of 3.x's input\n",
    "        initial_words = input(\"Please the word: \")\n",
    "    except ValueError:\n",
    "        print(\"Sorry, METAL SONGS DO NOT USE THIS POPPY WORDS!\")\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "initial_words = initial_words.split(' ')\n",
    "\n",
    "METAL_text = predict(device, net, initial_words, n_vocab,\n",
    "                    vocab_to_int, int_to_vocab, top_k=5)\n",
    "\n",
    "print((METAL_text.strip().lower()\n",
    "    .replace(' \\n ', '\\n')\n",
    "    .replace(' ! ', '! ')\n",
    "    .replace(' ? ', '? ')\n",
    "    .replace(' , ', ', ')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
